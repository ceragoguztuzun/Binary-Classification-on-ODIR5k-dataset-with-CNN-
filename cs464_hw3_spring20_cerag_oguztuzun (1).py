# -*- coding: utf-8 -*-
"""CS464_HW3_Spring20_Cerag_Oguztuzun.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y1FucEBiavnncz0go5p95dydrTD7Cccn

<h1><center>CS 464</center></h1>
<h1><center>Introduction to Machine Learning</center></h1>
<h1><center>Spring 2020</center></h1>
<h1><center>Homework 3</center></h1>

<h3><center>Due: May 19, 2020 23:55 (GMT+3)</center></h3>

LINK TO MODEL for MLP:
https://drive.google.com/open?id=1py9Q9N2-tlZd0MUcqxnAL6Or8mFZ8xQJ

LINK TO MODEL for CNN:
https://drive.google.com/open?id=1wSDlRO49rtSWaORtkTXS_8f2iD2_nMz6

## Question 2 [70 pts]

In this question you are asked to perform binary classification on Ocular Disease Recognition, ODIR5k, dataset. First, you will implement a three-layer neural network, and then a 3 layer convolutional neural network (CNN) to classify retinal images of 3500 patiens as either "normal" or "abnormal/disease".<br><br>
The dataset has been preprocessed in such a way that the right and the left retinal images from the same patient are combined, downsized and stored as H:128 X W:256 RGB images. The label for each patient is also given to you in another .xlsx file.
"""

import numpy as np
import pandas as pd
import torch
from torch import nn
import torch.nn.functional as F
from torchvision import transforms, datasets, models
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import PIL
from PIL import Image
import os
import cv2      
import glob
import random
from random import seed
from random import sample
import time
      

from google.colab import drive
drive.mount('/content/gdrive')

device = torch.device('cuda:0' if torch.cuda.is_available() else "cpu")

"""### 2.1. Multi Layer Perceptron (MLP) [30 pts]

#### Data Loader [7pts]

An important part of such a task is to implement your own data loader. In this homework, a partial loader is provided to you. This loader is going to be based on a base class named "Dataset", provided in PyTorch library. You need to complete the code below to create your custom "OcularDataset" class which will be able to load your dataset. <br><br>
Implement the functions whose proptotypes are given. Follow the TODO notes below. You have to divide the files into three sets as <b>train (5/7)</b>, <b>validation (1/7)</b> and **test (1/7)** sets.  These non-overlapping splits, which are subsets of OcularDataset, should be retrieved using the "get_dataset" function. Here, you are also supposed to flatten the image into a vector (also to grayscale) to be compatible with MLP. Note that the pixel values also needs to be normalized to [0,1] range.
<br><br>

Hint: The dataset is not normalized and your results will heavily depend on your input.
"""

class OcularDataset(Dataset):
    # TODO:
    # Define constructor for AnimalDataset class
    # HINT: You can pass processed data samples and their ground truth values as parameters 
    def __init__(self,x,y):
      x_t = torch.from_numpy(np.array(x, dtype = np.float32))
      y_t = torch.from_numpy(np.array(y, dtype = np.float32))
      self.data = x_t.cuda()
      self.labels = y_t.cuda()
      
    '''This function should return sample count in the dataset'''
    def __len__(self):
      return self.data.shape[0]

    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''
    def __getitem__(self, index):
      return self.data[index], self.labels[index]
    

def get_dataset():
    # TODO: 
    # Read dataset files
    # Construct training (5/7), validation (1/7) and test (1/7) sets
    # Normalize & flatten datasets

    #load images and labels from files
    labels = pd.read_excel('/content/gdrive/My Drive/Colabs/MLHW3/labels.xlsx')
    y = np.array(labels)
    y_ = []
    for i in range(0, y.shape[0]):
      y_.append(y[i][1])
    y = y_
    
    x = []
    img_dir = "/content/gdrive/My Drive/Colabs/MLHW3/images" 
    for directory in os.listdir(img_dir):
        data = mpimg.imread(os.path.join(img_dir, directory))
        if data is not None:
            x.append(data)
   
    # shuffle
    # everyday im shufflin'
    size = 3500
    weights = [0.2989, 0.5870, 0.1140]
    shuffled_x = []
    shuffled_y = []
    seed(1)
    index_array = random.sample(range(size), size)
    for i in range(0,size):
      shuffled_x.append(x[index_array[i]])
      shuffled_y.append(y[index_array[i]])

    xdata = []
    for i in range(size):
      pixels = shuffled_x[i]
      # normalize and greyscale pixels
      pixels = np.dot(pixels/255, weights)
      # flatten
      pixels = pixels.flatten()
      xdata.append(pixels)
    print("dataset set")

    # Construct training (5/7), validation (1/7) and test (1/7) sets
    size = len(y)
    size_of_train = int(size * (5/7))

    y_train = y[0:size_of_train]
    x_train = xdata[0:size_of_train]
    train_dataset = [x_train, y_train]

    size_of_val = int(size * (1/7))
    y_validation = y[size_of_train: size_of_train + size_of_val]
    x_validation = xdata[size_of_train: size_of_train + size_of_val]
    val_dataset = [x_validation, y_validation]

    y_test = y[size_of_train + size_of_val: size_of_train + size_of_val*2]
    x_test = xdata[size_of_train + size_of_val: size_of_train + size_of_val*2]
    test_dataset = [x_test, y_test]
    
    return train_dataset, val_dataset, test_dataset

"""#### Neural Network [4 pts]

Now, implement your three hidden layer neural network. FNet class will represent your neural network. The layer descriptions are as follows:<ul>
    <i>> Input layer will have ReLU activation. You should decide the number of input neurons.</i><br>
    <i>> First hidden layer will have 1024 neuros with ReLU activation </i><br>
    <i>> Second hidden layer will have 256 neuros with ReLU activation </i><br>
    <i> You should decide the number of output neurons and pick a proper activation function for the output layer. </i><br>
</ul>
"""

class FNet(nn.Module):
    '''Define your neural network'''
    def __init__(self, input_size, hiddenlayer1_size, hiddenlayer2_size, output_size): 
    # you can add any additional parameters you want 
    # TODO:
    # You should create your neural network here
      super(FNet, self).__init__()
      self.linear = nn.Linear(input_size, hiddenlayer1_size)
      self.linear1 = nn.Linear(hiddenlayer1_size, hiddenlayer2_size)
      self.linear2 = nn.Linear(hiddenlayer2_size, output_size)

    def forward(self, X): 
    # you can add any additional parameters you want
    # TODO:
    # Forward propagation implementation should be here
      X = F.relu(self.linear(X))
      X = F.relu(self.linear1(X))
      X = torch.sigmoid(self.linear2(X))
      return X

def predict(y):
  for i in range(len(y)):
    if y[i] > 0.35:
      y[i] = 1
    else:
      y[i] = 0
  return y

"""#### Training [10 pts]

Complete the code snippet below to train your network. You need to carefully select the appropriate loss function and tune hyper-parameters. Use SGD optimizer for this question.<br>
So far, you should have created three dataset splits for train, validation and test. You will need to load these splits at this phase. Make sure that you shuffle the samples in the training split. Save training loss and training accuracy of each iteration (each batch) and also save validation loss and accuracy at each epoch to use them in the next part for plotting.<br>
Your model is going to run upto at most 100 epochs. Pick the best model so far as your final model and save this model as a ".pth" file. <br>
Note that the best accuracy does not always imply the best model. Try to track losses instead of accuracies.
"""

#HINT: note that your training time should not take many days.

#TODO:
#Pick your hyper parameters
max_epoch = 100
train_batch = 30 
test_batch = 30
validation_batch = 30
learning_rate = 0.001

tr_acc_store = []
tr_loss_store = []
ts_acc_store = []
ts_loss_store = []

#use_gpu = torch.cuda.is_available()

def main(): # you are free to change parameters

    # Create train dataset loader
    # Create validation dataset loader
    # Create test dataset loader
    traintuple, validationtuple, testtuple = get_dataset()
    trainloader = DataLoader( OcularDataset( traintuple[0], traintuple[1]),
                                            batch_size = train_batch,
                                            shuffle = True) 
    testloader = DataLoader( OcularDataset( testtuple[0], testtuple[1]),
                                           batch_size = test_batch)
    valloader = DataLoader( OcularDataset( validationtuple[0], validationtuple[1]),
                                           batch_size = validation_batch)
    
    # initialize your GENet neural network
    torch.manual_seed(123)
    model = FNet(32768, 1024, 256, 1).to(device)
    #print(list(model.parameters()))

    # define your loss function
    criterion = nn.BCELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well
    
    # start training
    # for each epoch calculate validation performance
    # save best model according to validation performance
    #print(trainloader)
    best_acc = 0
    for epoch in range(max_epoch):
      print("--------------------EPOCH ",epoch,"--------------------")
      losses_train, accuracies_train = train(epoch, model, criterion, optimizer, trainloader)
      losses_test, accuracies_test = test(model, valloader, criterion)

      print( "train accuracy: ", np.mean(accuracies_train))
      print( "test accuracy: ", np.mean(accuracies_test))
      
      tr_acc_store.append(np.mean(accuracies_train))
      ts_acc_store.append(np.mean(accuracies_test))

      print( "train loss: ", losses_train)
      print( "test loss: ", losses_test)

      tr_loss_store.append(losses_train)
      ts_loss_store.append(losses_test)

      if np.mean(accuracies_test) > best_acc:
        best_acc = np.mean(accuracies_test)
        torch.save(model, "bestpath.pth")
    print("best accuracy reached: ", best_acc)
    
''' Train your network for a one epoch '''
def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters
    start_dt = time.time()
    model.train()
    stop_dt = time.time()
    data_time = stop_dt - start_dt

    losses = 0
    accuracies = []
    for batch_idx, (data, labels) in enumerate(loader):#batch_idx, (data, labels) in enumerate(loader):
      start_bt = time.time()
      # TODO:
      # Implement training code for a one iteration
      data = data.to(device)
      labels = labels.to(device)
        
      # loss
      predicted_label = model.forward(data)
      loss = criterion( predicted_label, labels.unsqueeze(1)) #added a dummy dimension
        
      # weight update
      loss.backward()
      optimizer.step()
      optimizer.zero_grad() 
      losses = losses + np.sum(np.array(loss.cpu().detach()))

      # making prediction
      predicted_label = predict(predicted_label)
        
      # accuracy
      t = sum(1 for a, b in zip(predicted_label[:], labels[:].unsqueeze(1)) if a == b)
      t = float(t)/25
      accuracies.append(t)
      stop_bt = time.time()
      batch_time = stop_bt - start_bt

    print("TRAIN Batch Time: ", batch_time)
    print("TRAIN Data Time: ", data_time)
      
    # divide by element sizee
    return losses/2500, np.asarray(accuracies)
    

''' Test&Validate your network '''
def test(model, loader,criterion): # you are free to change parameters
    start_dt = time.time()
    model.eval()
    stop_dt = time.time()
    data_time = stop_dt - start_dt

    losses = 0
    accuracies = []
    with torch.no_grad():
      for batch_idx, (data, labels) in enumerate(loader):
        start_bt = time.time()
        # TODO:
        # Implement test code
        data = data.to(device)
        labels = labels.to(device)

        # prediction & loss
        prediction = model.forward(data)
        loss = criterion(prediction, labels.unsqueeze(1))
        losses = losses + np.sum(np.array(loss.cpu().detach())) 

        prediction = predict( prediction)
   
        # accuracy
        t = sum(1 for a, b in zip(prediction[:], labels[:].unsqueeze(1)) if a == b)
        t = float(t)/25
        accuracies.append(t)
        stop_bt = time.time()
        batch_time = stop_bt - start_bt
      print("TEST Batch Time: ", batch_time)
      print("TEST Data Time: ", data_time)
         
      # divide by element size
      return losses/500, np.array(accuracies)

main()

#plot of losses
def plotLosses(loss_train, loss_test): 
  plt.title("Plot of Losses")
  plt.plot(loss_train, label = "Train Loss", color= "red")
  plt.plot(loss_test,  label = "Test Loss", color= "green")
  plt.legend()
  plt.grid()
  plt.show()

def plotAccuracies(accuracies_train, accuracies_test):
  plt.title("Plot of Accuracies")
  plt.plot(accuracies_train, label = "Train Accuracy", color= "red")
  plt.plot(accuracies_test,  label = "Test Accuracy", color= "green")
  plt.legend()
  plt.grid()
  plt.show()

"""#### Plotting Your Results [4 pts]

You need to provide two distinct plots, one demonstrating training and validation losses in y axis and iteration in the x axis and the other demonstrating training and validation accuracies in the y axis and iteration  in the x axis. <br><br>
Please note that we need these plots to see if your model behaves as expected. Therefore, you may lose additional points if you do not provide these plots.
"""

plotLosses(tr_loss_store, ts_loss_store)

plotAccuracies(tr_acc_store, ts_acc_store)

"""#### Testing [5 pts]

Test your final, i.e. best, model on your test set. Calculate confusion matrix, F1 score, precision and recall values and report these findings.
"""

# write your code in this cell to test your best model with the test dataset
traintuple, validationtuple, testtuple = get_dataset()
test_load = DataLoader( OcularDataset( testtuple[0], testtuple[1]),
                                        batch_size = 30)
model = torch.load('bestpath.pth')
criterion = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well

def testing(model, loader):
  model.eval()
  preds = []
  labels = []
  tp = 1  #in order to avoid a case of division by zero
  fp = 1
  fn = 1
  tn = 1
  with torch.no_grad():
    for batch_idx, (data, labels) in enumerate(loader):
      # TODO:
      # Implement test code
      data = data.to(device)
      labels = labels.to(device)

      # prediction
      prediction = model.forward(data)
      prediction = predict( prediction)

      # confusion matrix calculation
      predictions = np.array(prediction[:].cpu().detach())
      thelabels = np.array(labels[:].unsqueeze(1).cpu().detach())

      tp += np.sum( np.logical_and( thelabels, predictions) )
      fp += np.sum( np.logical_and( np.logical_not(thelabels), predictions) )
      fn += np.sum( np.logical_and( thelabels, np.logical_not(predictions)) )
      tn += np.sum( np.logical_and( np.logical_not(thelabels), np.logical_not(predictions)) )
    
    precision = tp / (tp+fp)
    recall = tp / (tp+fn)
    f1 = 2*(precision*recall) / (precision+recall)
    
    #outputs
    print("TP: ", tp, "FP: ", fp, "FN: ",fn, "TN: ",tn)
    print("Precision: ", precision)
    print("Recall: ", recall)
    print("F1: ", f1)


testing(model, test_load)

"""### 2.2. Convolutional Neural Network (CNN) [30 pts]

#### Data Loader [5 pts]

In this part, you will train a CNN for the same problem. Again, the pixel values need to be normalized to [0,1] range. Please do **not** change images to grayscale this time. First, implement the data loader (OcularDataset). You have to divide the files into three sets which are <b>train (5/7)</b>, <b>validation (1/7)</b> and **test (1/7)**.  These non-overlapping splits, which are subsets of OcularDataset, should be retrieved using the "get_dataset" function.<br> You may use your data loader from the previous sections with propoer modifications. Note that this time you do **not** need to flatten the image.
"""

class OcularDataset(Dataset):
    # TODO:
    # Define constructor for SVHNDataset class
    # HINT: You can pass processed data samples and their ground truth values as parameters 
    def __init__(self,x,y):
      x_t = torch.from_numpy(np.array(x, dtype = np.float32))
      y_t = torch.from_numpy(np.array(y, dtype = np.float32))
      self.data = x_t.cuda()
      self.labels = y_t.cuda()

    '''This function should return sample count in the dataset'''
    def __len__(self):
      return self.data.shape[0]

    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''
    def __getitem__(self, index):
      return self.data[index, :], self.labels[index]

        
def get_dataset():
    # TODO: 
    # Read dataset files
    # Construct training (5/7), validation (1/7) and test (1/7) sets
    # Normalize datasets

    #load images and labels from files
    labels = pd.read_excel('/content/gdrive/My Drive/Colabs/MLHW3/labels.xlsx')
    y = np.array(labels)
    y_ = []
    for i in range(0, y.shape[0]):
      y_.append(y[i][1])
    y = y_
    
    x = []
    img_dir = "/content/gdrive/My Drive/Colabs/MLHW3/images" 
    for directory in os.listdir(img_dir):
        data = mpimg.imread(os.path.join(img_dir, directory))
        if data is not None:
            x.append(data)
   
    # shuffle
    # everyday im shufflin'
    size = 3500
    shuffled_x = []
    shuffled_y = []
    seed(1)
    index_array = random.sample(range(size), size)
    for i in range(0,size):
      shuffled_x.append(x[index_array[i]])
      shuffled_y.append(y[index_array[i]])

    xdata = []
    for i in range(size):
      pixels = shuffled_x[i]
      # normalize pixels
      pixels = pixels/255
      xdata.append(pixels)
    print("dataset set")
    
    # Construct training (5/7), validation (1/7) and test (1/7) sets
    size = len(y)
    size_of_train = int(size * (5/7))

    y_train = y[0:size_of_train]
    x_train = xdata[0:size_of_train]
    train_dataset = [x_train, y_train]

    size_of_val = int(size * (1/7))
    y_validation = y[size_of_train: size_of_train + size_of_val]
    x_validation = xdata[size_of_train: size_of_train + size_of_val]
    val_dataset = [x_validation, y_validation]

    y_test = y[size_of_train + size_of_val: size_of_train + size_of_val*2]
    x_test = xdata[size_of_train + size_of_val: size_of_train + size_of_val*2]
    test_dataset = [x_test, y_test]

    return train_dataset, val_dataset, test_dataset

"""#### Convolutional Neural Network [8 pts]

Now implement your CNN. ConvNet class will represent your convolutional neural network. Implement 3 layers of convolution: 
1. <i>> 8 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br> 
    <i>> ReLU </i><br>
2. <i>> 16 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br>
    <i>> ReLU </i><br>
    <i>> MaxPool 2 x 2 </i><br>   
3. <i>> 32 filters with size of 3 x 3 x 3 with stride 1 and no padding,</i><br>
    <i>> ReLU </i><br>
    <i>> MaxPool 2 x 2 </i><br>

As a classification layer, you need to add only one more fully-connected layer at the end of the network. You need to choose the appropriate input and output neuron sizes and the activation function.
"""

class ConvNet(nn.Module):
    '''Define your neural network'''
    def __init__(self): # you can add any additional parameters you want 
    # TODO:
    # You should create your neural network here
      super().__init__()
      #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,
      self.convolution1 = nn.Conv2d(in_channels=3, out_channels=8, 
                                    kernel_size=3, stride=1, padding=0) 
      self.convolution2 = nn.Conv2d(in_channels=8, out_channels=16, 
                                    kernel_size=3, stride=1, padding=0) 
      self.convolution3 = nn.Conv2d(in_channels=16, out_channels=32, 
                                    kernel_size=3, stride=1, padding=0)

      self.fc1 = nn.Linear(32*62*30, 1)#(122*250*32, 1)

      self.sigmoid = nn.Sigmoid()
      self.maxpool2x2 = nn.MaxPool2d(2)

    def forward(self, X): # you can add any additional parameters you want
    # TODO:
    # Forward propagation implementation should be here
      X = self.convolution1(X)
      X = F.relu(X)

      X = self.convolution2(X)
      X = F.relu(X)
      X = self.maxpool2x2(X)
    
      X = self.convolution3(X)
      X = F.relu(X)
      X = self.maxpool2x2(X)

      X = X.view(-1,32*62*30)
      X = self.fc1(X)
      X = F.log_softmax(X)
      return X

"""#### Training and Testing [17 pts]

Now, train your network. You need to select the appropriate loss function and your hyper-parameters.<br>
Make sure to shuffle the samples in the training split.<br>
 Plot the training and validation loss for each iteration. Also plot the training  and validation accuracy as another figure.<br>
  Your model is going to run upto the "max_epoch" parameter. Pick the best model as your final model and save this model as a ".pth" file. Note that the best accuracy does not always imply the best model. Try to track losses instead of accuracies. <br>
  Report the test performance change (In terms of accuracy, F1 score, precision and recall) between MLP and CNN and explain the reason for this change explicitly, if there is any.
"""

# HINT: note that your training time should not take many days.

# TODO:
# Pick your hyper parameters
max_epoch = 100
train_batch = 30
test_batch = 30
val_batch = 30
learning_rate = 0.001
tr_acc_store = []
tr_loss_store = []
ts_acc_store = []
ts_loss_store = []
input_size = [256,128]
kernel_size = 3
#use_gpu = torch.cuda.is_available()

def predict(y):
  for i in range(len(y)):
    if y[i] > 0.35:
      y[i] = 1
    else:
      y[i] = 0
  return y

def main(): # you are free to change parameters

    # load data
    traintuple, validationtuple, testtuple = get_dataset()

    # format data
    traintuple[0] = np.array(traintuple[0]).reshape(2500, kernel_size, input_size[0], input_size[1])
    validationtuple[0] = np.array(validationtuple[0]).reshape(500, kernel_size, input_size[0], input_size[1])
    testtuple[0] = np.array(testtuple[0]).reshape(500, kernel_size, input_size[0], input_size[1])
    traintuple[1] = np.array(traintuple[1]).reshape(2500,1)
    validationtuple[1] = np.array(validationtuple[1]).reshape(500,1)
    testtuple[1] = np.array(testtuple[1]).reshape(500,1)
    
    # Create train dataset loader
    # Create validation dataset loader
    # Create test dataset loader
    trainloader = DataLoader( OcularDataset( traintuple[0], traintuple[1]),
                                            batch_size = train_batch,
                                            shuffle = True) 
    testloader = DataLoader( OcularDataset( testtuple[0], testtuple[1]),
                                           batch_size = test_batch)
    valloader = DataLoader( OcularDataset( validationtuple[0], validationtuple[1]),
                                           batch_size = val_batch)

    # initialize your GENet neural network
    model = ConvNet().to(device)

    # define your loss function
    criterion = nn.BCELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well
    
    # start training
    # for each epoch calculate validation performance
    # save best model according to validation performance
    
    best_acc = 0
    for epoch in range(max_epoch):
      print("--------------------EPOCH ",epoch,"--------------------")
      losses_train, accuracies_train = train(epoch, model, criterion, optimizer, trainloader)
      losses_test, accuracies_test = test(model, valloader, criterion)
   
      print( "train accuracy: ", np.mean(accuracies_train))
      print( "test accuracy: ", np.mean(accuracies_test))
      print( "train loss: ", losses_train)
      print( "test loss: ", losses_test)

      tr_acc_store.append(np.mean(accuracies_train))
      tr_loss_store.append(losses_train)
      ts_acc_store.append(np.mean(accuracies_test))
      ts_loss_store.append(losses_test)

      if np.mean(accuracies_test) > best_acc:
        best_acc = np.mean(accuracies_test)
        torch.save(model, "bestpath_cnn.pth")
    print("best accuracy reached: ", best_acc)
    
''' Train your network for a one epoch '''
def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters
    start_dt = time.time()
    model.train()
    stop_dt = time.time()
    data_time = stop_dt - start_dt

    accuracies = []
    losses = 0
    for batch_idx, (data, labels) in enumerate(loader):
      start_bt = time.time()
      # TODO:
      # Implement training code for a one iteration
      data = data.cuda()#to(device)
      labels = labels.cuda()#to(device)
      # loss
      predicted_label = model.forward(data)
      loss = criterion( predicted_label, labels.unsqueeze(1)) #added a dummy dimension
      # weight update
      loss.backward()
      optimizer.step()
      optimizer.zero_grad() 
      losses = losses + np.sum(np.array(loss.cpu().detach()))
      # making prediction
      predicted_label = predict(predicted_label)
      # accuracy
      t = sum(1 for a, b in zip(predicted_label[:], labels[:].unsqueeze(1)) if a == b)
      t = float(t)/25
      accuracies.append(t)
      stop_bt = time.time()
    print("TRAIN Batch Time: ", stop_bt - start_bt)
    print("TRAIN Data Time: ", data_time)
    # divide by element sizee
    return losses/2500, np.asarray(accuracies)

''' Test&Validate your network '''
def test(model, loader, criterion): # you are free to change parameters
    start_dt = time.time()
    model.eval()
    stop_dt = time.time()
    data_time = stop_dt - start_dt

    losses = 0
    accuracies = []
    with torch.no_grad():
      for batch_idx, (data, labels) in enumerate(loader):
        start_bt = time.time()
        # TODO:
        # Implement test code
        data = data.to(device)
        labels = labels.to(device)

        # prediction & loss
        prediction = model.forward(data)
        loss = criterion(prediction, labels.unsqueeze(1))
        losses = losses + np.sum(np.array(loss.cpu().detach())) 

        prediction = predict( prediction)
           
        # accuracy
        t = sum(1 for a, b in zip(prediction[:], labels[:].unsqueeze(1)) if a == b)
        t = float(t)/25
        accuracies.append(t)
        stop_bt = time.time()
      print("TEST Batch Time: ", stop_bt - start_bt)
      print("TEST Data Time: ", data_time)
      
      # divide by element size
      return losses/500, np.array(accuracies)

main()

"""PLOTS"""

plotLosses(tr_loss_store, ts_loss_store)

plotAccuracies(tr_acc_store, ts_acc_store)

"""TESTING"""

# write your code in this cell to test your best model with the test dataset
input_size = [256,128]
kernel_size = 3

traintuple, validationtuple, testtuple = get_dataset()
testtuple[0] = np.array(testtuple[0]).reshape(500, kernel_size, input_size[0], input_size[1])
traintuple[1] = np.array(traintuple[1]).reshape(2500,1)
test_load = DataLoader( OcularDataset( testtuple[0], testtuple[1]),
                                        batch_size = 30)
model = torch.load('bestpath_cnn.pth')
criterion = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well

def testing(model, loader):
  model.eval()
  preds = []
  labels = []
  tp = 1
  fp = 1
  fn = 1
  tn = 1
  with torch.no_grad():
    for batch_idx, (data, labels) in enumerate(loader):
      # TODO:
      # Implement test code
      data = data.to(device)
      labels = labels.to(device)

      # prediction
      prediction = model.forward(data)
      prediction = predict( prediction)

      # confusion matrix calculation
      predictions = np.array(prediction[:].cpu().detach())
      thelabels = np.array(labels[:].unsqueeze(1).cpu().detach())

      tp += np.sum( np.logical_and( thelabels, predictions) )
      fp += np.sum( np.logical_and( np.logical_not(thelabels), predictions) )
      fn += np.sum( np.logical_and( thelabels, np.logical_not(predictions)) )
      tn += np.sum( np.logical_and( np.logical_not(thelabels), np.logical_not(predictions)) )

    precision = tp / (tp+fp)
    recall = tp / (tp+fn)
    f1 = (2*precision*recall) / (precision+recall)
    
    #outputs
    print("TP: ", tp, "FP: ", fp, "FN: ",fn, "TN: ",tn)
    print("Precision: ", precision)
    print("Recall: ", recall)
    print("F1: ", f1)

testing(model, test_load)

"""### 2.3 Interpretation [10 pts.]

Explicitly discuss the results that you have obtained in Question 2. <ul>
    > Among MLP and CNN , which one do you think is better? <br>
    > What are the weaknesses and strengths of each method?<br>
    > Why do we use max pooling layers for CNN? What would happen if we used average pooling instead? <br>
    > How can we interpret the weights of convolutional layers? <br>
</ul>

CNN was better than MLP in this dataset, because from results we can see that we are able to achieve a higher accuracy and F1 score with CNN than we did with MLP.

MLP has a downside that when the number of parameters increase for fully connected layers, It created inefficiency. Another downside of MLP is that It takes flattened images which makes us neglect some information of the dataset. It's positive side is that, It is able to distinguish data that is not able to be seperated linearly and It is simpler than CNN.

CNN has smaller weights so It is more easily trained then MLP. CNN can take matrices as inputs which makes it better than MLP. CNNs downside is that It is computationally expensive and It is prone to overfitting.

We use max pooling to reduce variance to reduce computational complexity in our code. We use max pooling layers because It is better at extracting more important features than average pooling does. But average pooling takes all the information into account and doesn't get rid of big chunks of data. If we used average pooling our accuracy would get lower.

Weights of convolutional layers correspond to a filter that is used to extract features from the input. This layer is moving in strides on the input which and padding can also be included. According to the weights initilized on the filter we can reduce the input image and only use the important features to do classification.

##References

Ocular Disease Recognition - ODIR5k Dataset (https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k)
"""